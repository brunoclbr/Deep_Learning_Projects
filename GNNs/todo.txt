alpha=				0.0001,		0.0001,		0.0001,		0.0001,
batch=				64,		64,		121,		72,	
epoch=				15,		25,		15,		15,
last_epoch-mae-val_mae=		(0.525, 0.558), (0.519, 0.573), (0.5209, 0.5761)(0.516, 0.571)
over_fitting_point[epoch]=	9(0.5617),	6(0.5563),	3(0.569),	6(0.573)

using first combination and then:

add 2 dropouts @ 0.25, no OF with 15 epochs, final mae/val_mae (0.5286, 0.5612) --> a bit of longer training but no real improvement w.r.t best case

add 2 dropouts @ 0.25, OF with 30 epochs @epoch10, mae/val_mae(0.520, 0,5704), final mae/val_mae (0.5286, 0.5612)

add 2 dropouts @ 0.75, OF with 15 epochs @epoch10, mae/val_mae(0.646, 0,569), final mae/val_mae (0.5522, 0.566)

add 2 dropouts @ 0.5, no OF with 15 epochs, final mae/val_mae (0.535, 0.555) --> SLIGTHLY BEST VAL MAE

add 2 dropouts @ 0.5, OF with 30 epochs, inital guess was very good so OF from the begining, final mae/val_mae (0.521, 0.568) 
------------------------------------

THEN I TRIED TO ELIMINATE MY CONTEXT IN THE MODEL SINCE I DONT HAVE ONE, SLIGTHLY WORSE RESULTS BUT MAKES MORE SENSE final mae/val_mae (0.537, 0.556) 

------------------------------------

ENOUGH OF HP. NOW FINISH MODEL INFERENCE, THEN GO TO SMILES, READY, !!!!!!!!! NOW LACKING COMPUTING POWER  !!!!!!

Learn this:
 YOUTUBE TOOL FOR HP check
 TENSORBOARD checkish
 Get Your first Model back from GitHub (or new branch from here and use saved keras model)
	--> recreate parity plot with updated metrics for baseline
	--> done, only that could not recreate first (best) results. Moving forward.
	--> i did change a lot with the removing of hydrogens...maybe there is the problem


toy akiiiiiiiiiiiiiiiiiiiiii
---------> SIGO AKI
COMPLEMENTAR DOCS CON AXIS=0 DE GPT QUE HICE EL OTRO DIA + HANDEL_MISSING_VALUES_STEP DE END-TO-END VIDEO.

Get Computing Power/Fix error

Send your files and see what happens --> try to make the 2 features model work, review the hydrogen part. It must beat baseline.

If better nice, if not fuck it.
 Can see how my youtube friend defines his SMILES for model training. Try to understand parallel procesing and
computing power as you reach limitations

Go to BO, if 2F model beats base line. Have something to show.

FINISH BY BEGINNING OF APRIL

COMING BACK FROM CHILE FOR GEN CNN 
<KerasTensor: type_spec=GraphTensorSpec({'context': ContextSpec({'features': {}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, tf.int64, None), 'node_sets': {'atoms': NodeSetSpec({'features': {'atomic_number': TensorSpec(shape=(None, 118), dtype=tf.float32, name=None), 'electro_negativity': TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)}, 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, tf.int64, None)}, 'edge_sets': {'bonds': EdgeSetSpec({'features': {'hidden_state': TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)}, 'adjacency': AdjacencySpec({'#index.0': TensorSpec(shape=(None,), dtype=tf.int32, name=None), '#index.1': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, tf.int64, {'#index.0': 'atoms', '#index.1': 'atoms'}), 'sizes': TensorSpec(shape=(1,), dtype=tf.int32, name=None)}, TensorShape([]), tf.int32, tf.int64, None)}}, TensorShape([]), tf.int32, tf.int64, None) (created by layer 'input.merge_batch_to_components')>

GraphTensorSpec({'context': ContextSpec({'features': {}, 'sizes': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)}, TensorShape([None]), tf.int32, tf.int64, None), 
'node_sets': {'atoms': NodeSetSpec({'features': {'atomic_number': RaggedTensorSpec(TensorShape([None, None, 118]), tf.float32, 1, tf.int64), 'electro_negativity': RaggedTensorSpec(TensorShape([None, None, 1]), tf.float32, 1, tf.int64)}, 'sizes': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)}, TensorShape([None]), tf.int32, tf.int64, None)},
 'edge_sets': {'bonds': EdgeSetSpec({'features': {'hidden_state': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float32, 1, tf.int64)}, 'adjacency': AdjacencySpec({'#index.0': RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64), '#index.1': RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)}, TensorShape([None]), tf.int32, tf.int64, {'#index.0': 'atoms', '#index.1': 'atoms'}), 'sizes': TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)}, TensorShape([None]), tf.int32, tf.int64, None)}}, TensorShape([None]), tf.int32, tf.int64, None)), TensorSpec(shape=(None, 1), dtype=tf.float32, name=None))>
